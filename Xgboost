
# coding: utf-8

# In[8]:

import numpy as np
from scipy import sparse
import pandas as pd
import xgboost as xgb
import bayes_opt
import re
import string
import time
import math
import random
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing, pipeline, metrics
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.decomposition import PCA, FastICA, TruncatedSVD
from sklearn.random_projection import GaussianRandomProjection
from sklearn.random_projection import SparseRandomProjection

def LabelEncoding(dataframe):
        LEVars = []
        le = LabelEncoder()
        #for dataframe in args:
        for column in dataframe.columns:
            if dataframe[column].dtype == 'object':
                tempLEVar = 'LE_' + column
                dataframe[tempLEVar] = le.fit_transform(dataframe[column])
                LEVars.append(tempLEVar)
                dataframe.drop(labels=column, axis=1, inplace=True)
        return dataframe, LEVars
    
def xgb_r2_score(preds, dtrain):
    # Courtesy of Tilii
    labels = dtrain.get_label()
    return 'r2', r2_score(labels, preds)

def Output(pred, testID):
    output = pd.DataFrame()
    output['ID'] = testID
    output['y'] = pred
    fileName = input("File name?")
    path = './Output/' + fileName + '.csv'
    output.to_csv(path, index=False)

#def average_dups(x):
#    # Average value of duplicates
#    Y.loc[list(x.index)] = Y.loc[list(x.index)].mean()
def CountingMissingVals(*args):
        Result = pd.DataFrame()
        for (idx, dataframe) in enumerate(args):
            MissingVal = dataframe.isnull().sum()
            Result[idx] = MissingVal
        return Result
        
def runXGB(booster_params, train_X, train_y, test_X, test_y=None, feature_names=None, \
           seed_val=2017, eta=0.05, num_rounds=5000):
        param = {}
        param['objective'] = booster_params['objective'] #'multi:softprob'
        param['eta'] = booster_params['eta']
        param['max_depth'] = booster_params['max_depth']
        #param['silent'] = 0
        #param['num_class'] = 1 # Think about target!
        #param['eval_metric'] = 'rmse' #"mlogloss"
        param['min_child_weight'] = booster_params['min_child_weight']
        param['subsample'] = booster_params['subsample']
        param['colsample_bytree'] = booster_params['colsample_bytree']
        param['gamma'] = booster_params['gamma'] #alias: min_split_loss
        param['seed'] = booster_params['seed']
        num_rounds = num_rounds

        #plst = list(param.items())
        xgtrain = xgb.DMatrix(train_X, label=train_y.reshape(train_X.shape[0],1), feature_names=feature_names)

        if test_y is not None:
            xgtest = xgb.DMatrix(test_X, label=test_y, feature_names=feature_names)
            watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]
            model = xgb.train(params=param, dtrain=xgtrain, num_boost_round=num_rounds, feval=xgb_r2_score,\
                              evals=watchlist, maximize=True, early_stopping_rounds=20)
        else:
            xgtest = xgb.DMatrix(test_X, feature_names=feature_names)
            model = xgb.train(param, xgtrain, num_boost_round=num_rounds, feval=xgb_r2_score)

        pred_test_y = model.predict(xgtest)
        return model, pred_test_y
    
def XGB_BayesianOptimizationParamsDict():
    
        tuning_params_dict = dict()
        #tuning_params_dict['eta'] = (0.01, 0.1)
        tuning_params_dict['gamma'] = (0,2)
        tuning_params_dict['max_depth'] = (4,8)
        tuning_params_dict['min_child_weight'] = (1,100)
        tuning_params_dict['subsample'] = (0.5,1)
        tuning_params_dict['colsample_bytree'] = (0.2, 0.8)
    
        return tuning_params_dict
    
def evaluateXGB(min_child_weight, colsample_bytree, max_depth, subsample, gamma):
    
        xgbCV = xgb.DMatrix(X, Y)
    
        # Here are non-tunable parameters.
    
        booster_params = dict()
        booster_params['objective'] = 'reg:linear'
        #booster_params['metric'] = 'rmse'
        #booster_params['num_class'] = 1 # This needs to be changed upon projects!
        #booster_params['stratified'] = True 
        booster_params['eta'] = 0.03
        booster_params['early_stopping_rounds'] = 20
        booster_params['verbose_eval'] = True
        booster_params['seed'] = 2017
        booster_params['num_boost_round'] = 5000
        booster_params['n_fold'] = 3
        #booster_params['folds'] = params_dict['folds'] # folds : a KFold or StratifiedKFold instance. Sklearn KFolds or StratifiedKFolds.    
        booster_params['silent'] = 0
    
        booster_params['max_depth'] = int(max_depth)
        booster_params['min_child_weight'] = int(min_child_weight)
        booster_params['colsample_bytree'] = float(colsample_bytree)
        booster_params['subsample'] = float(subsample)
        booster_params['gamma'] = float(gamma)
    
        XGB_cv_result = xgb.cv(params=booster_params, 
                           dtrain=xgbCV,
                           num_boost_round = booster_params['num_boost_round'],
                           nfold = 3,
                           #stratified=False,
                           feval = xgb_r2_score,
                           maximize=True, # This may be intergrable with Bayesian Optimization.
                           early_stopping_rounds = 20,
                           verbose_eval = False, # If want to see the CV result of every rounds, turn this on.
                           seed = 2017
                          )
    
        return XGB_cv_result['test-r2-mean'].max()
    
def BO_XGB(function=evaluateXGB, params_dict=XGB_BayesianOptimizationParamsDict()):
    
        xgb_BO = bayes_opt.BayesianOptimization(f=function, pbounds=params_dict)
        xgb_BO.maximize(init_points=5, n_iter=30)
    
        ## Show tuning results
        xgb_BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])
        xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])
        xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)
        xgb_BO_scores.head()
    
        return xgb_BO_scores
        #return xgb_BO
    
def blending_XGB_models(results, train_X, train_y, test_X, test_y=None, num_models=4):
    
        params_list = []
        scores = []
        print("Loading parameters...")
        for row in results.head(num_models).iterrows():
        
            if row[1]['score'] in scores:
                continue
                scores.append(row[1]['score'])
        
            booster_params = dict()
            booster_params['objective'] = 'reg:linear'
            #booster_params['metric'] = 'rmse'
            #booster_params['num_class'] = 1 # This needs to be changed upon projects!
            #booster_params['stratified'] = True 
            booster_params['eta'] = 0.01
            #booster_params['early_stopping_rounds'] = 50
            #booster_params['verbose_eval'] = True
            booster_params['seed'] = 2017
            #booster_params['num_boost_round'] = 1000000
            #booster_params['n_fold'] = 5
            #booster_params['folds'] = params_dict['folds'] # folds : a KFold or StratifiedKFold instance. Sklearn KFolds or StratifiedKFolds.    
            booster_params['silent'] = 0
    
            booster_params['max_depth'] = int(row[1].to_dict()['max_depth'])
            booster_params['min_child_weight'] = int(row[1].to_dict()['min_child_weight'])
            booster_params['colsample_bytree'] = row[1].to_dict()['colsample_bytree']
            booster_params['subsample'] = row[1].to_dict()['subsample']
            booster_params['gamma'] = row[1].to_dict()['gamma']
        
            params_list.append(booster_params)
            scores.append(row[1]['score'])
        
        xgbTrain = xgb.DMatrix(train_X, label=train_y)
        xgbTrainCV = xgb.DMatrix(train_X)
        xgbTest = xgb.DMatrix(test_X)
        
        print("Making predictions...")
        XGBModels = []
        XGB_pred = pd.DataFrame()
        for idx, param in enumerate(params_list):
            print("Blending model %i..." %(idx+1))
            model, pred = runXGB(booster_params, train_X, train_y, test_X)
            #model = xgb.train(params=param, dtrain=xgbTrain, num_boost_round=5000)
            XGBModels.append(model)
            XGB_pred[idx] = pred
        
        print("Blending complete.")
        return XGBModels, XGB_pred
        


# In[169]:



#ohe = OneHotEncoder()
#result2 = ohe.fit_transform(train_df['X0'])

#train_df.columns[0]


# In[136]:

#train_data, TrainLEVars= MercedesBenz.LabelEncoding(train_df)
#test_data, TestLEVars = MercedesBenz.LabelEncoding(test_df)


# In[139]:

#train_data.drop(labels=['ID'], axis = 1, inplace = True)
#test_data.drop(labels=['ID'], axis =1 , inplace=True)
