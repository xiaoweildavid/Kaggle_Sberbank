# Sberbank Russian Housing Market

# Here are some useful links:

# Naive XGB in Python: https://www.kaggle.com/matthewa313/naive-xgb-in-python

import numpy as np
from scipy import sparse
import pandas as pd
import xgboost as xgb
import lightgbm as lgb
import re
import string
import time
from bayes_opt import BayesianOptimization

from sklearn import preprocessing, pipeline, metrics, model_selection
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn import feature_selection
from itertools import product

import matplotlib.pyplot as plt
#%matplotlib inline 

train_df = pd.read_csv('./input/Sberbank/train.csv')
test_df = pd.read_csv('./input/Sberbank/test.csv')
macro_df = pd.read_csv('./input/Sberbank/macro.csv')

train_size = train_df.shape[0]

print('\nSome examples of dataset...\n')
train_df.head(3)

full_df = pd.concat([train_df.drop(labels='price_doc', axis=1),test_df], axis = 0)

print('\nIn Case that the original data needs to be recovered...\n')
full_df_backup = full_df

print('\nThere are a lof of missing values...\n')
full_df.isnull().sum().sort_values(ascending=False)

full_df[full_df['build_year']>=2020][['build_year','state']]

print('\nNumber of missing values of state is: ', full_df[np.isnan(full_df['state'])].shape[0])
print('\nThere are so many missing states, maybe judged by build year...\n')

print('\nDealing with missing values...\n')

temp_df[temp_df['build_year']==2015]['state'].describe()
temp_df[temp_df['build_year']==215]['state'].describe()
print('\nFrom above comparison, probabaly build_year 215 is actually 2015.\n')
temp_df[temp_df['build_year']==71]['state'].describe()
temp_df[temp_df['build_year']==1971]['state'].describe()
print('\nFrom above comparison, probabaly build_year 71 is actually 1971.\n')

temp_df[temp_df['build_year']==0]

temp = full_df[full_df['build_year']<1860]
for year in temp['build_year'].unique().tolist():
    print('For build_year = %s, the records number is %s.' %(year, full_df[full_df['build_year']==year].shape[0]))

temp_df = full_df
temp_df = temp_df.reset_index()

for i in temp_df[temp_df['build_year']<1860].index:
    temp_row = temp_df.loc[i]
    temp_full_sq = temp_row['full_sq']
    temp_sub_area = temp_row['sub_area']
    temp_metro_min_avto = temp_row['metro_min_avto']
    temp_year = temp_row['build_year']
    
    if temp_year == 215:
        temp_df.loc[i, 'build_year'] = 2015
        continue

    if temp_year == 71:
        temp_df.loc[i, 'build_year'] = 1971
        continue
        
    if temp_year == 0 or temp_year == 1:
        if temp_row['full_sq'] == temp_row['life_sq']:
            temp_df.loc[i,'build_year'] = 2018
            continue
            
    # This is an outlier:
    if temp_year == 2:
        temp_df.loc[i, 'build_year'] = 2018
        continue

    candidates = temp_df[(temp_df['full_sq']==temp_full_sq) & (temp_df['sub_area']==temp_sub_area)
                        & (temp_df['metro_min_avto']==temp_metro_min_avto)] #选出符合条件的数据。
    
    candidates = candidates[(candidates['build_year']!=temp_year) & 
                            (candidates['build_year']!=0) &
                            (candidates['build_year']!=1)].dropna()
    
    if candidates.shape[0] >= 1:
        temp_df.loc[i, 'build_year'] = candidates['build_year'].sum()/candidates.shape[0]
    
print('\nAfter the modification, the missing values are:\n')
temp = temp_df[temp_df['build_year']<1860]
for year in temp['build_year'].unique().tolist():
    print('For build_year = %s, the records number is %s.' %(year, temp_df[temp_df['build_year']==year].shape[0]))
    
temp = temp_df[temp_df['build_year']<1860]
for year in temp['build_year'].unique().tolist():
    print('For build_year = %s, the records number is %s.' %(year, temp_df[temp_df['build_year']==year].shape[0]))
    
for i in temp_df[temp_df['build_year']<10].index:
    
    temp_max_floor = temp_df.loc[i, 'max_floor']
    
    if np.isnan(temp_max_floor) == False and temp_max_floor>=13:
        temp_df.loc[i,'build_year'] = round(temp_df[(temp_df['max_floor']>=temp_max_floor) & (temp_df['build_year']>10)]['build_year'].mean())
        continue
    elif temp_df.loc[i, 'floor'] >= 9:     
        temp_df.loc[i,'build_year'] = round(temp_df[(temp_df['floor']>=temp_df.loc[i, 'floor']) & (temp_df['build_year']>10)]['build_year'].mean())
    
    
print('\n')
temp = temp_df[temp_df['build_year']<1860]
for year in temp['build_year'].unique().tolist():
    print('For build_year = %s, the records number is %s.' %(year, temp_df[temp_df['build_year']==year].shape[0]))    
    
basic_vars = full_df.columns[3:11].tolist()
print('\nBasic features: ', basic_vars,'\n')

def FillMissingVals(df=full_df, features=basic_vars):
    print('\nNow dealing with missing values...\n')
    df = df.reset_index()
    #for i in df.index[:20]: # This one is for tuning the program.
    for i in df.index:
        temp_row = df.loc[i]
        temp_full_sq = temp_row['full_sq']
        temp_sub_area = temp_row['sub_area']
        temp_metro_min_avto = temp_row['metro_min_avto']
        #print(temp_row,'\n',temp_full_sq)
        candidates = df[(df['full_sq']==temp_full_sq) & (df['sub_area']==temp_sub_area)
                        & (df['metro_min_avto']==temp_metro_min_avto)]
    
        for var in features:
            if np.isnan(temp_row[var]) == True:
                if candidates[var].dropna().shape[0] >= 1:
                    #print('Changing the %f records...' %(full_df.iloc[i]['id']))
                    df.loc[i, var] = round(candidates[var].dropna().sum()/candidates[var].dropna().shape[0])
                    #print(candidates[var].dropna())
                
    print('All done!')
    
    return df

print('\nNow statistically investigating some key features...\n')
print('\nprice_doc: sale price (this is the target variable)\n')
print(train_df['price_doc'].describe())
print('\nfull_sq: total area in square meters, including loggias, balconies and other non-residential areas\n')
print(full_df['full_sq'].describe())
print('\nlife_sq: living area in square meters, excluding loggias, balconies and other non-residential areas\n')
print(full_df['life_sq'].describe())
print('\nbuild_year: year built\n')
print(full_df['build_year'].describe())
print('\nnum_room: number of living room\n')
print(full_df['num_room'].describe())
print('\nkitch_sq: kitchen area')
print(full_df['kitch_sq'].describe())

print('\nFind out categorical features...\n')
cat_vars = ['product_type','sub_area','ecology']

for ele in full_df[:1]:
    #print(ele,':', full_df.iloc[0][ele])
    if full_df.iloc[0][ele] == 'no' or full_df.iloc[0][ele] == 'yes':
        cat_vars.append(ele)
print(cat_vars, '\n\nThere are %s categorical features.' %(len(cat_vars)))
print('\nValidating...\n')
full_df[cat_vars].head(3)
#for ele in cat_vars:
#    print(ele,':', full_df.iloc[0][ele])

print('\nEnable LabelEncoder for labeling the categorical features...\n')

def EncodingCatVars():
    LBL = preprocessing.LabelEncoder()

    LE_vars=[]
    LE_map=dict()
    for cat_var in cat_vars:
        print ("Label Encoding %s ..." % (cat_var))
        LE_var=cat_var+'_le'
        #print(full_df[cat_var])
        full_df[LE_var]=LBL.fit_transform(full_df.fillna('Not_Available')[cat_var])
        LE_vars.append(LE_var)
        LE_map[cat_var]=LBL.classes_
        print ('Done!')
    
    #print ("Label-encoded feaures: %s" % (LE_vars))
    
    return LE_vars, LE_map
    
print('\nLabel-encoding categorical features...\n')
LE_vars, LE_map = EncodingCatVars()
full_df[LE_vars].head(3)

num_vars = full_df.drop(labels=(cat_vars+LE_vars+['timestamp']), axis=1).columns.tolist()
print('\nNow dealing with outliers in some features...\n')
print('\nThis part will be done later...\n')
print('\nNow combining all variables...\n')
full_vars = num_vars + LE_vars
print('\nPreparing data...\n')
print('\nDrop timestamp... Needs more work on this feature...\n')
#train_X = sparse.hstack([train_df[features_to_use], tr_sparse]).tocsr()
#test_X = sparse.hstack([test_df[features_to_use], te_sparse]).tocsr()

#target_num_map = {'high':0, 'medium':1, 'low':2}
#train_y = np.array(train_df['interest_level'].apply(lambda x: target_num_map[x]))
#print(train_X.shape, test_X.shape)

full_df.fillna(value=-1, inplace=True)

train_X = sparse.csr_matrix(full_df[full_vars].values)[:train_size]
train_y = train_df['price_doc'].values

test_X = sparse.csr_matrix(full_df[full_vars].values)[train_size:]

features = full_df[full_vars].columns

def runXGB(train_X, train_y, test_X, test_y=None, feature_names=None, seed_val=2017, num_rounds=100000):
    param = {}
    param['objective'] = 'reg:linear' #'multi:softprob'
    param['eta'] = 0.03
    param['max_depth'] = 8
    param['silent'] = 1
    param['num_class'] = 1 # Think about target!
    param['eval_metric'] = 'rmse' #"mlogloss"
    param['min_child_weight'] = 6
    param['subsample'] = 0.7 
    param['colsample_bytree'] = 0.7
    param['gamma'] = 1.8 # This did not exist in gdy5's notebook. alias: min_split_loss
    param['seed'] = seed_val
    num_rounds = num_rounds

    #plst = list(param.items())
    xgtrain = xgb.DMatrix(train_X, label=train_y.reshape(train_X.shape[0],1), feature_names=features)

    if test_y is not None:
        xgtest = xgb.DMatrix(test_X, label=test_y, feature_names=features)
        watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]
        model = xgb.train(params=param, dtrain=xgtrain, num_boost_round=num_rounds, 
                          evals=watchlist, early_stopping_rounds=20)
    else:
        xgtest = xgb.DMatrix(test_X, feature_names=features)
        model = xgb.train(param, xgtrain, num_rounds)

    pred_test_y = model.predict(xgtest)
    return model, pred_test_y


def runLGB(train_x, train_y, test_x, test_y=None, feature_names=None, seed_val=2017, num_boost_round=100000):
    params = {}
    params['objective'] = 'mse' #'multiclass'
    params['metric'] = 'l2' #'multi_logloss'
    params['num_class'] = 1 # This must be changed! Maybe also for XGB...
    params['learning_rate'] = 0.03
    params['max_bins'] = 1024   
    params['num_leaves'] = 512    
    params['min_sum_hessian_in_leaf'] = 1
    params['min_gain_to_split'] = 0
    params['feature_fraction'] = 0.8
    params['bagging_fraction'] = 0.7
    params['bagging_freq'] = 1
    params['seed'] = seed_val
    num_boost_round = num_boost_round
    
    LGBtrain = lgb.Dataset(train_x,label=train_y)
    
    if test_y is not None:
        LGBtest = lgb.Dataset(test_x, label=test_y)
        watchlist = [LGBtrain, LGBtest]
        watchlist_name = ['train','test']
        model = lgb.train(params=params, train_set=LGBtrain, num_boost_round=num_boost_round,
                          valid_sets=watchlist, 
                          valid_names=watchlist_name,
                          early_stopping_rounds=20)
    else:
        #LGBtest  = lgb.Dataset(test_x)
        model = lgb.train(params=params, train_set=LGBtrain, num_boost_round=num_boost_round)
        
    pred_LGB_test_y = model.predict(test_x)
    return model, pred_LGB_test_y
    
print('\nNow tuning parameters... Bayesian Optimization.\n')

def evaluateXGB(min_child_weight, colsample_bytree, max_depth, subsample, gamma):
    
    xgbtrain = xgb.DMatrix(train_X, label=train_y.reshape(train_X.shape[0],1))
    
    # Here are non-tunable parameters.
    
    booster_params = dict()
    booster_params['objective'] = 'reg:linear'
    booster_params['metric'] = 'rmse'
    booster_params['num_class'] = 1 # This needs to be changed upon projects!
    #booster_params['stratified'] = True 
    booster_params['eta'] = 0.1
    booster_params['early_stopping_rounds'] = 50
    booster_params['verbose_eval'] = True
    booster_params['seed'] = 2017
    booster_params['num_boost_round'] = 1000000
    booster_params['n_fold'] = 5
    #booster_params['folds'] = params_dict['folds'] # folds : a KFold or StratifiedKFold instance. Sklearn KFolds or StratifiedKFolds.    
    booster_params['silent'] = 1
    
    booster_params['max_depth'] = int(max_depth)
    booster_params['min_child_weight'] = int(min_child_weight)
    booster_params['colsample_bytree'] = float(colsample_bytree)
    booster_params['subsample'] = float(subsample)
    booster_params['gamma'] = float(gamma)
    
    XGB_cv_result = xgb.cv(params=booster_params, 
                           dtrain=xgbtrain,
                           num_boost_round = booster_params['num_boost_round'],
                           nfold = 5,
                           #stratified=False,
                           metrics = 'rmse',
                           #maximize=Flase, # This may be intergrable with Bayesian Optimization.
                           early_stopping_rounds = 20,
                           verbose_eval = False, # If want to see the CV result of every rounds, turn this on.
                           seed = 2017
                          )
    
    return -XGB_cv_result['test-rmse-mean'].min()
    
    
def BayesianOptimizationParamsDict():
    
    tuning_params_dict = dict()
    #tuning_params_dict['eta'] = (0.01, 0.1)
    tuning_params_dict['gamma'] = (0,2)
    tuning_params_dict['max_depth'] = (4,9)
    tuning_params_dict['min_child_weight'] = (1,100)
    tuning_params_dict['subsample'] = (0.7,1)
    tuning_params_dict['colsample_bytree'] = (0.2, 0.7)
    
    return tuning_params_dict

def BO_XGB(function=evaluateXGB, params_dict=BayesianOptimizationParamsDict()):
    
    xgb_BO = BayesianOptimization(function, params_dict)
    xgb_BO.maximize(init_points=5, n_iter=30)
    
    ## Show tuning results
    xgb_BO_scores = pd.DataFrame(xgb_BO.res['all']['params'])
    xgb_BO_scores['score'] = pd.DataFrame(xgb_BO.res['all']['values'])
    xgb_BO_scores = xgb_BO_scores.sort_values(by='score',ascending=False)
    #xgb_BO_scores.head()
    
    return xgb_BO_scores
    
print('\nNow running models...\n')
cv_scores = []
kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=2017)
model_dict = {}

count = 0
for dev_index, val_index in kf.split(range(train_X.shape[0])):
        print("\nIteration number: %s \n" %(count+1))
        dev_X, val_X = train_X[dev_index,:], train_X[val_index,:]
        dev_y, val_y = train_y[dev_index], train_y[val_index]
        model, preds = runXGB(dev_X, dev_y, val_X, val_y)
        #model, preds = runLGB(dev_X, dev_y, val_X, val_y)
        temp_score = metrics.mean_squared_error(val_y, preds)
        model_dict[str(temp_score)] = model
        cv_scores.append(temp_score)
        print(cv_scores)
        count += 1
        #break

# Averaging the prediction results.

#cv_scores.sort()
xgbtest = xgb.DMatrix(test_X, feature_names=features)
predictions = np.zeros(test_X.shape[0])

print('\nPlotting the feature importance...\n')

for ele in cv_scores:
    fig, ax = plt.subplots(1, 1, figsize=(8, 13))
    xgb.plot_importance(model, height=0.5, ax=ax) # max_num_features=50, 
    model = model_dict[str(ele)]
    preds = model.predict(xgbtest)
    predictions = predictions + preds.reshape(-1,)

preds = predictions/len(cv_scores)
output = pd.DataFrame({'id': test_df['id'], 'price_doc': preds})

output.to_csv("./output/Sberbank/XGB_single_model.csv", index=False)


