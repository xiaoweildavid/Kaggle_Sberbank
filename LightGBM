
# coding: utf-8

# In[8]:

import numpy as np
from scipy import sparse
import pandas as pd
import xgboost as xgb
import bayes_opt
import re
import string
import time
import math
import random
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn import preprocessing, pipeline, metrics
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.ensemble import RandomForestRegressor
from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder
from sklearn.decomposition import PCA, FastICA, TruncatedSVD
from sklearn.random_projection import GaussianRandomProjection
from sklearn.random_projection import SparseRandomProjection

def runLGB(train_x, train_y, test_x, test_y=None, feature_names=None, seed_val=2017, num_boost_round=100000):
        params = {}
        params['objective'] = 'regression' #'multiclass'
        #params['metric'] = 'l2' #'multi_logloss'
        #params['num_class'] = 1 # This must be changed! Maybe also for XGB...
        params['learning_rate'] = 0.03
        params['max_bins'] = 1024   
        params['num_leaves'] = 512    
        params['min_sum_hessian_in_leaf'] = 1
        params['min_gain_to_split'] = 0
        params['feature_fraction'] = 0.7
        params['bagging_fraction'] = 0.7
        params['bagging_freq'] = 1
        params['seed'] = seed_val
        num_boost_round = num_boost_round
    
        LGBtrain = lgb.Dataset(train_x,label=train_y)
    
        if test_y is not None:
            LGBtest = lgb.Dataset(test_x, label=test_y)
            watchlist = [LGBtrain, LGBtest]
            watchlist_name = ['train','test']
            model = lgb.train(params=params, train_set=LGBtrain, num_boost_round=num_boost_round,
                          valid_sets=watchlist, 
                          valid_names=watchlist_name,
                          early_stopping_rounds=20)
            return model, pred_LGB_test_y
    
        else:
            LGBtest  = lgb.Dataset(test_x)
            model = lgb.train(params=params, train_set=LGBtrain, num_boost_round=num_boost_round,
                         early_stopping_rounds=20)
            pred_LGB_test_y = model.predict(LGBtest)
            return model, pred_LGB_test_y
        
def LGB_BayesianOptimizationParamsDict():
    tuning_params_dict = dict()
    tuning_params_dict['max_bins'] = (64,1024)
    tuning_params_dict['num_leaves'] = (2, 512)
    tuning_params_dict['min_sum_hessian_in_leaf'] = (1,100)
    tuning_params_dict['min_gain_to_split'] = (0,2)
    tuning_params_dict['feature_fraction'] = (0.2, 0.8)
    tuning_params_dict['bagging_fraction'] = (0.5, 1)
    tuning_params_dict['bagging_freq'] = (1,5)
    
    return tuning_params_dict

def evaluateLGB(max_bins, num_leaves, min_sum_hessian_in_leaf, min_gain_to_split, \
                feature_fraction, bagging_fraction, bagging_freq):
    
    lgbCV = lgb.Dataset(x_train, y_train)
    
    # Here are non-tunable parameters.
    
    params = dict()
    params['objective'] = 'mse' #'multiclass'
    params['metric'] = 'l2' #'multi_logloss'
#    params['num_class'] = 1 # This must be changed! Maybe also for XGB...
    params['num_boost_round'] = 1000000
    params['learning_rate'] = 0.1
    params['num_class'] = 1
    
    params['max_bins'] = int(max_bins)   
    params['num_leaves'] = int(num_leaves)    
    params['min_sum_hessian_in_leaf'] = int(min_sum_hessian_in_leaf)
    params['min_gain_to_split'] = int(min_gain_to_split)
    params['feature_fraction'] = feature_fraction
    params['bagging_fraction'] = bagging_fraction
    params['bagging_freq'] = int(bagging_freq)
    params['seed'] = 2017
    
    LGB_cv_result = lgb.cv(params=params, 
                           train_set=lgbCV,
                           num_boost_round = params['num_boost_round'],
                           nfold = 5,
                           #stratified=False,
                           metrics = 'rmse',
                           #maximize=Flase, # This may be intergrable with Bayesian Optimization.
                           early_stopping_rounds = 20,
                           verbose_eval = False, # If want to see the CV result of every rounds, turn this on.
                           seed = seed_val
                          )
    
    return -pd.DataFrame(LGB_cv_result)
